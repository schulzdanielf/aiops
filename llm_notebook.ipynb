{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnóstico de Aplicação com LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-25 13:24:00.326813: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740500640.348577   63537 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740500640.353915   63537 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-25 13:24:00.376785: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515fe3a15cd04e818fcabadb1e320300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Carregar o modelo DeepSeek R1 Distill Llama em 8-bit\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,  # Quantização 8-bit\n",
    "    device_map=\"auto\"   # Distribui o modelo automaticamente entre CPU/GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faça 3 consultas no prometheus para as seguintes métricas: requests_total, errors_total e latency\n",
    "\n",
    "# Importe a biblioteca prometheus_client\n",
    "from prometheus_client import start_http_server, Summary, Counter\n",
    "from prometheus_client.core import REGISTRY\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Faça a requisição para o servidor Prometheus\n",
    "def query_prometheus(query):\n",
    "    response = requests.get(\"http://localhost:30090/api/v1/query\", params={\"query\": query})\n",
    "    data = json.loads(response.text)\n",
    "    return data[\"data\"][\"result\"]\n",
    "\n",
    "\n",
    "def get_prometheus_metrics():\n",
    "    # Consulta para a métrica requests_total com a média dos últimos 5 minutos\n",
    "    requests_total = query_prometheus(\"avg(rate(requests_total[5m]))*60\")\n",
    "    requests_total = round(float(requests_total[0][\"value\"][1]), 2)\n",
    "\n",
    "    # Consulta para a métrica errors_total com a média dos últimos 5 minutos\n",
    "    errors_total = query_prometheus(\"avg(rate(errors_total[5m]))*60\")\n",
    "    errors_total = round(float(errors_total[0][\"value\"][1]), 2)\n",
    "\n",
    "    # Consulta para a métrica latency com a média dos últimos 5 minutos\n",
    "    latency = query_prometheus(\"rate(latency_milliseconds_sum[5m])\")\n",
    "    latency = round(float(latency[0][\"value\"][1]), 2)\n",
    "\n",
    "    return requests_total, errors_total, latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latência: 5.3 segundos, trafego: 53.27, taxa de erros: 4.63%\n"
     ]
    }
   ],
   "source": [
    "# Testando a requisição no Prometheus\n",
    "\n",
    "requests_total, errors_total, latency = get_prometheus_metrics()\n",
    "\n",
    "texto = f\"Latência: {latency} segundos, trafego: {requests_total}, taxa de erros: {errors_total}%\"\n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63537/2825321465.py:101: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=local_llm, prompt=prompt)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "class GPTJLocal(LLM):\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        # Exemplos de few-shot learning para melhorar a qualidade das respostas\n",
    "        few_shot_examples = \"\"\"\n",
    "        Você é um assistente de AIOps. Sua tarefa é analisar as métricas de desempenho de uma aplicação, comparar as métricas atuais com o histórico e responder à pergunta de forma clara e precisa. Siga os seguintes passos:\n",
    "        1. Analise cada métrica individualmente.\n",
    "        2. Compare com o que é esperado.\n",
    "        3. Identifique problemas ou anomalias.\n",
    "        4. Conclua com uma avaliação geral.\n",
    "\n",
    "        Métricas normais para esta aplicação (SLO):\n",
    "        - Latência: Menor que 10 segundos\n",
    "        - Taxa de erros: Menor que 20%\n",
    "        - Trafego: Menos de 100 requisições por segundo\n",
    "\n",
    "        Exemplo 1:\n",
    "        Métricas atuais: Latência: 20.39 segundos, trafego: 48.63, taxa de erros: 6.74%\n",
    "        Pergunta: Avalie minha aplicação. Está tudo bem?\n",
    "        Resposta:\n",
    "        Conclusão: A aplicação não está saudável devido à alta latência. Recomendo investigar a causa do aumento.\n",
    "\n",
    "        Exemplo 2:\n",
    "        Métricas atuais: Latência: 4.39 segundos, trafego: 48.63, taxa de erros: 6.74%\n",
    "        Pergunta: Avalie minha aplicação. Está tudo bem?\n",
    "        Resposta:\n",
    "        Conclusão: A aplicação está saudável e com desempenho dentro do esperado.\n",
    "\n",
    "        Exemplo 3:\n",
    "        Métricas atuais: Latência: 40.39 segundos, trafego: 480.63, taxa de erros: 6.74%\n",
    "        Pergunta: Avalie minha aplicação. Está tudo bem?\n",
    "        Resposta:\n",
    "        Conclusão: A aplicação não está saudável devido a alta latencia. Provavelmente ocorrendo devido ao aumento do trafego.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Combinar os exemplos de few-shot com o prompt do usuário\n",
    "        full_prompt = few_shot_examples + f\"\\n\\nMétricas atuais: {prompt}\\nResposta:\"\n",
    "\n",
    "        # Tokenizar a entrada\n",
    "        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # Gerar texto com parâmetros ajustados\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=1024,\n",
    "                temperature=0.2,  # Controla a criatividade (valores mais baixos = mais determinístico)\n",
    "                top_p=0.8,        # Amostragem nucleus (controla a diversidade)\n",
    "                top_k=40,         # Limita o vocabulário às top-k palavras\n",
    "                do_sample=True,     # Ativa a amostragem\n",
    "                num_beams=3,       # Geração de feixe para melhorar a coerência\n",
    "                early_stopping=True,          # Para a geração quando a resposta estiver completa\n",
    "                pad_token_id=tokenizer.eos_token_id  # Usa o token de fim de texto para parar\n",
    "            )\n",
    "        \n",
    "        # Decodificar a saída\n",
    "        output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extrair apenas a resposta gerada (ignorar o prompt)\n",
    "        response = output_text.split(\"Resposta:\")[-1].strip()\n",
    "\n",
    "        # Verificar se a resposta está completa\n",
    "        if \"Conclusão:\" not in response:\n",
    "            response += \"\\nConclusão: A aplicação precisa de uma análise mais detalhada.\"\n",
    "\n",
    "        return response\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"name_of_model\": \"DeepSeek-R1-Distill-Llama-8B\"}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"Llama\"\n",
    "\n",
    "# Instanciar o modelo local\n",
    "local_llm = GPTJLocal()\n",
    "\n",
    "# Definir o prompt\n",
    "template = \"\"\"\n",
    "Você é um assistente de AIOps. Analise as métricas de desempenho de uma aplicação abaixo, compare as atuais com o histórico e responda à pergunta. Siga os seguintes passos:\n",
    "1. Analise cada métrica individualmente.\n",
    "2. Compare com o que é esperado.\n",
    "3. Identifique diferenças nas métricas, problemas ou anomalias.\n",
    "4. Conclua com uma avaliação geral.\n",
    "\n",
    "Métricas atuais: {texto}\n",
    "Pergunta: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"texto\", \"question\"])\n",
    "\n",
    "# Criar a cadeia\n",
    "chain = LLMChain(llm=local_llm, prompt=prompt)\n",
    "\n",
    "# Função para interpretar o estado da aplicação\n",
    "def interpret_application_state(texto, question):\n",
    "    response = chain.run(texto=texto, question=question)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Situação de Métricas Normais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latência: 4.3 segundos, trafego: 50.32, taxa de erros: 5.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_63537/2825321465.py:105: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(texto=texto, question=question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conclusão: A aplicação está saudável e com desempenho dentro do esperado.\n",
      "</think>\n",
      "\n",
      "Conclusão: A aplicação está saudável e com desempenho dentro do esperado.\n"
     ]
    }
   ],
   "source": [
    "requests_total, errors_total, latency = get_prometheus_metrics()\n",
    "\n",
    "texto = f\"Latência: {latency} segundos, trafego: {requests_total}, taxa de erros: {errors_total}%\"\n",
    "print(texto)\n",
    "question = \"Avalie minha aplicação. Está tudo bem?\"\n",
    "\n",
    "# Interpretar o estado da aplicação\n",
    "response = interpret_application_state(texto, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevando a taxa de erros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latência: 4.63 segundos, trafego: 49.9, taxa de erros: 41.050000000000004%\n",
      "**\n",
      "\n",
      "Conclusão: Apesar da latência e trafego estarem dentro dos limites, a taxa de erros elevada é um sinal de alerta. Precisamos investigar o que está causando o\n"
     ]
    }
   ],
   "source": [
    "requests_total, errors_total, latency = get_prometheus_metrics()\n",
    "\n",
    "# Multiplicando os erros por 5\n",
    "errors_total = errors_total * 5\n",
    "\n",
    "texto = f\"Latência: {latency} segundos, trafego: {requests_total}, taxa de erros: {errors_total}%\"\n",
    "print(texto)\n",
    "question = \"Faça um diagnóstico da minha aplicação.\"\n",
    "\n",
    "# Interpretar o estado da aplicação\n",
    "response = interpret_application_state(texto, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elevando o tráfego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latência: 5.63 segundos, trafego: 207.16, taxa de erros: 8.0%\n",
      "Conclusão: A aplicação está saudável e com desempenho dentro do\n"
     ]
    }
   ],
   "source": [
    "requests_total, errors_total, latency = get_prometheus_metrics()\n",
    "\n",
    "# Multiplicando o tráfego  por 4\n",
    "requests_total = requests_total * 4\n",
    "\n",
    "texto = f\"Latência: {latency} segundos, trafego: {requests_total}, taxa de erros: {errors_total}%\"\n",
    "print(texto)\n",
    "question = \"Como está minha aplicação?\"\n",
    "\n",
    "# Interpretar o estado da aplicação\n",
    "response = interpret_application_state(texto, question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Próximos passos\n",
    "\n",
    "- Buscar métricas de banco de dados\n",
    "- Buscar métricas do servidor de aplicação\n",
    "- Tentar identificar se o problema é no banco de dados ou na aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
